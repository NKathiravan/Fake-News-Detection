{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "Qnl6SvZvNouO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas torch torchtext scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MIvkhBuNoJG",
        "outputId": "f41cf18f-3ebc-45c9-b771-b2b96f6ddaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#datasets"
      ],
      "metadata": {
        "id": "wCuy-5QaNsYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZGfpYMRMynE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the TSV files\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv', sep='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv', sep='\\t', header=None)\n",
        "test_df  = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv',  sep='\\t', header=None)\n",
        "\n",
        "# Set proper column names\n",
        "columns = ['id', 'label', 'statement', 'subjects', 'speaker', 'job_title',\n",
        "           'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts',\n",
        "           'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
        "\n",
        "train_df.columns = columns\n",
        "valid_df.columns = columns\n",
        "test_df.columns = columns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSYCSoUeSOqG",
        "outputId": "e07a6eef-2138-4aa0-b35d-85d4cbdb2244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           id  label                                          statement  \\\n",
            "0   2635.json      1  Says the Annies List political group supports ...   \n",
            "1  10540.json      3  When did the decline of coal start? It started...   \n",
            "2    324.json      4  Hillary Clinton agrees with John McCain \"by vo...   \n",
            "3   1123.json      1  Health care reform legislation is likely to ma...   \n",
            "4   9028.json      3  The economic turnaround started at the end of ...   \n",
            "\n",
            "                             subjects         speaker             job_title  \\\n",
            "0                            abortion    dwayne-bohac  State representative   \n",
            "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
            "2                      foreign-policy    barack-obama             President   \n",
            "3                         health-care    blog-posting                   NaN   \n",
            "4                        economy,jobs   charlie-crist                   NaN   \n",
            "\n",
            "  state_info party_affiliation  barely_true_counts  false_counts  \\\n",
            "0      Texas        republican                 0.0           1.0   \n",
            "1   Virginia          democrat                 0.0           0.0   \n",
            "2   Illinois          democrat                70.0          71.0   \n",
            "3        NaN              none                 7.0          19.0   \n",
            "4    Florida          democrat                15.0           9.0   \n",
            "\n",
            "   half_true_counts  mostly_true_counts  pants_on_fire_counts  \\\n",
            "0               0.0                 0.0                   0.0   \n",
            "1               1.0                 1.0                   0.0   \n",
            "2             160.0               163.0                   9.0   \n",
            "3               3.0                 5.0                  44.0   \n",
            "4              20.0                19.0                   2.0   \n",
            "\n",
            "               context  \n",
            "0             a mailer  \n",
            "1      a floor speech.  \n",
            "2               Denver  \n",
            "3       a news release  \n",
            "4  an interview on CNN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess"
      ],
      "metadata": {
        "id": "iYQFfrYGNvfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping labels to numbers\n",
        "label_mapping = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 0,\n",
        "    'barely-true': 0,\n",
        "    'half-true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'true': 1\n",
        "}\n",
        "\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "valid_df['label'] = valid_df['label'].map(label_mapping)\n",
        "test_df['label'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# Drop rows with missing labels or statements\n",
        "train_df.dropna(subset=['statement', 'label'], inplace=True)\n",
        "valid_df.dropna(subset=['statement', 'label'], inplace=True)\n",
        "test_df.dropna(subset=['statement', 'label'], inplace=True)\n"
      ],
      "metadata": {
        "id": "5IWGVfkwN8cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenize"
      ],
      "metadata": {
        "id": "1NqC6zKGOCMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "\n",
        "# Simple tokenizer\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Build vocabulary\n",
        "all_text = train_df['statement'].tolist()\n",
        "counter = Counter()\n",
        "for text in all_text:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "# Keep top words\n",
        "vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common(10000))}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# Encode a sentence\n",
        "def encode(text):\n",
        "    return [vocab.get(word, vocab['<UNK>']) for word in tokenize(text)]\n",
        "\n",
        "# Custom Dataset\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = [torch.tensor(encode(text)) for text in df['statement']]\n",
        "        self.labels = torch.tensor(df['label'].tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Pad sequences inside the batch\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return texts, labels\n",
        "\n",
        "train_dataset = FakeNewsDataset(train_df)\n",
        "valid_dataset = FakeNewsDataset(valid_df)\n",
        "test_dataset  = FakeNewsDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "1r7ukUwqOEwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN Model"
      ],
      "metadata": {
        "id": "4Yaf7dm7OJqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FakeNewsRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(FakeNewsRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        out = self.fc(hidden.squeeze(0))\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 6\n",
        "\n",
        "model = FakeNewsRNN(vocab_size, embed_dim, hidden_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "0vHd7oWaOLlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "-NdlOolpOOdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "    result()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DQyzhv4OQNF",
        "outputId": "8fbc6383-d630-433c-e0f2-51bb47e8f6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.7130\n",
            "Accuracy: 20.36%\n",
            "Epoch 2, Loss: 1.7107\n",
            "Accuracy: 21.31%\n",
            "Epoch 3, Loss: 1.7057\n",
            "Accuracy: 22.42%\n",
            "Epoch 4, Loss: 1.7017\n",
            "Accuracy: 22.97%\n",
            "Epoch 5, Loss: 1.6884\n",
            "Accuracy: 23.05%\n",
            "Epoch 6, Loss: 1.6645\n",
            "Accuracy: 21.94%\n",
            "Epoch 7, Loss: 1.6500\n",
            "Accuracy: 22.02%\n",
            "Epoch 8, Loss: 1.6745\n",
            "Accuracy: 20.76%\n",
            "Epoch 9, Loss: 1.7019\n",
            "Accuracy: 20.84%\n",
            "Epoch 10, Loss: 1.7354\n",
            "Accuracy: 18.00%\n",
            "Epoch 11, Loss: 1.7452\n",
            "Accuracy: 18.15%\n",
            "Epoch 12, Loss: 1.7048\n",
            "Accuracy: 21.39%\n",
            "Epoch 13, Loss: 1.6873\n",
            "Accuracy: 20.28%\n",
            "Epoch 14, Loss: 1.6698\n",
            "Accuracy: 20.60%\n",
            "Epoch 15, Loss: 1.6769\n",
            "Accuracy: 20.44%\n",
            "Epoch 16, Loss: 1.6757\n",
            "Accuracy: 22.02%\n",
            "Epoch 17, Loss: 1.7098\n",
            "Accuracy: 20.76%\n",
            "Epoch 18, Loss: 1.6833\n",
            "Accuracy: 21.78%\n",
            "Epoch 19, Loss: 1.6557\n",
            "Accuracy: 22.10%\n",
            "Epoch 20, Loss: 1.6508\n",
            "Accuracy: 22.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "b4M6s_EXOSD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def result():\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for texts, labels in test_loader:\n",
        "          texts, labels = texts.to(device), labels.to(device)\n",
        "          outputs = model(texts)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "id": "iUQRXACtOTby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#naive bayes"
      ],
      "metadata": {
        "id": "jq0XkBuUUGED"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 1. Load your train, validation, and test data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv', sep='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv', sep='\\t', header=None)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv', sep='\\t', header=None)\n",
        "\n",
        "# 2. Set column names (based on your description)\n",
        "columns = [\n",
        "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\",\n",
        "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
        "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "]\n",
        "\n",
        "train_df.columns = columns\n",
        "valid_df.columns = columns\n",
        "test_df.columns = columns\n",
        "\n",
        "label_mapping = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 0,\n",
        "    'barely-true': 0,\n",
        "    'half-true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'true': 1\n",
        "}\n",
        "\n",
        "# Map the 'label' column using the label_mapping dictionary\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "valid_df['label'] = valid_df['label'].map(label_mapping)\n",
        "test_df['label'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "\n",
        "# 3. Combine train + valid for final training (optional but better)\n",
        "full_train_df = pd.concat([train_df, valid_df])\n",
        "\n",
        "# 4. Get text and labels\n",
        "X_train = full_train_df['statement']\n",
        "y_train = full_train_df['label']\n",
        "\n",
        "X_test = test_df['statement']\n",
        "y_test = test_df['label']\n",
        "\n",
        "# 5. Text Vectorization (TF-IDF)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# 6. Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 7. Predict\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# 8. Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6SGIDwmVGUs",
        "outputId": "3e1699aa-a104-4c4d-8f9a-d75315634780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.611681136543015\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.39      0.47       553\n",
            "           1       0.62      0.78      0.69       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.59      0.58      1267\n",
            "weighted avg       0.61      0.61      0.60      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#svm randomforest xgboost gdboost"
      ],
      "metadata": {
        "id": "RanG49d4VlRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 1. Load your train, validation, and test data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv', sep='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv', sep='\\t', header=None)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv', sep='\\t', header=None)\n",
        "\n",
        "# 2. Set column names\n",
        "columns = [\n",
        "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\",\n",
        "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
        "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "]\n",
        "\n",
        "train_df.columns = columns\n",
        "valid_df.columns = columns\n",
        "test_df.columns = columns\n",
        "\n",
        "# 3. Label mapping: group into 0 (false) and 1 (true)\n",
        "label_mapping = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 0,\n",
        "    'barely-true': 0,\n",
        "    'half-true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'true': 1\n",
        "}\n",
        "\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "valid_df['label'] = valid_df['label'].map(label_mapping)\n",
        "test_df['label'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# 4. Combine train + valid for full training\n",
        "full_train_df = pd.concat([train_df, valid_df])\n",
        "\n",
        "# 5. Get text and labels\n",
        "X_train = full_train_df['statement']\n",
        "y_train = full_train_df['label']\n",
        "\n",
        "X_test = test_df['statement']\n",
        "y_test = test_df['label']\n",
        "\n",
        "# 6. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# 7. Choose your model\n",
        "# 7.1 SVM\n",
        "model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 9. Predict\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# 10. Evaluate\n",
        "print(\"svm\\n\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 7.2 Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 9. Predict\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# 10. Evaluate\n",
        "print(\"random forest\\n\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 7.3 XGBoost\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 9. Predict\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# 10. Evaluate\n",
        "print(\"xgboost\\n\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 7.4 Gradient Boosting\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# 8. Train the model\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 9. Predict\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# 10. Evaluate\n",
        "print(\"GD boost\\n\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# 7.5 KNN\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 9. Predict\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# 10. Evaluate\n",
        "print(\"random forest\\n\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k_cHXwpVskL",
        "outputId": "aa093555-547d-4f1e-df01-6f271aa015c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "svm\n",
            "\n",
            "Accuracy: 0.6069455406471981\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.45      0.50       553\n",
            "           1       0.63      0.73      0.68       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.59      0.59      1267\n",
            "weighted avg       0.60      0.61      0.60      1267\n",
            "\n",
            "random forest\n",
            "\n",
            "Accuracy: 0.6108918705603789\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.40      0.47       553\n",
            "           1       0.63      0.77      0.69       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.59      0.58      1267\n",
            "weighted avg       0.60      0.61      0.60      1267\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:14:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost\n",
            "\n",
            "Accuracy: 0.6093133385951065\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.39      0.46       553\n",
            "           1       0.62      0.78      0.69       714\n",
            "\n",
            "    accuracy                           0.61      1267\n",
            "   macro avg       0.60      0.58      0.58      1267\n",
            "weighted avg       0.60      0.61      0.59      1267\n",
            "\n",
            "GD boost\n",
            "\n",
            "Accuracy: 0.6045777426992897\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.24      0.35       553\n",
            "           1       0.60      0.88      0.72       714\n",
            "\n",
            "    accuracy                           0.60      1267\n",
            "   macro avg       0.61      0.56      0.53      1267\n",
            "weighted avg       0.61      0.60      0.56      1267\n",
            "\n",
            "random forest\n",
            "\n",
            "Accuracy: 0.5777426992896606\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.14      0.23       553\n",
            "           1       0.58      0.91      0.71       714\n",
            "\n",
            "    accuracy                           0.58      1267\n",
            "   macro avg       0.57      0.53      0.47      1267\n",
            "weighted avg       0.57      0.58      0.50      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#voting model"
      ],
      "metadata": {
        "id": "rIVU6YyHcuri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 1. Load your train, validation, and test data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv', sep='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv', sep='\\t', header=None)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv', sep='\\t', header=None)\n",
        "\n",
        "# 2. Set column names\n",
        "columns = [\n",
        "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\",\n",
        "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
        "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "]\n",
        "train_df.columns = columns\n",
        "valid_df.columns = columns\n",
        "test_df.columns = columns\n",
        "\n",
        "# 3. Label mapping\n",
        "label_mapping = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 0,\n",
        "    'barely-true': 0,\n",
        "    'half-true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'true': 1\n",
        "}\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "valid_df['label'] = valid_df['label'].map(label_mapping)\n",
        "test_df['label'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# 4. Combine train + valid\n",
        "full_train_df = pd.concat([train_df, valid_df])\n",
        "\n",
        "# 5. Get text and labels\n",
        "X_train = full_train_df['statement']\n",
        "y_train = full_train_df['label']\n",
        "X_test = test_df['statement']\n",
        "y_test = test_df['label']\n",
        "\n",
        "# 6. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# 7. Create the models\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# 8. Voting Classifier\n",
        "voting_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('svm', svm_model),\n",
        "        ('rf', rf_model),\n",
        "        ('xgb', xgb_model)\n",
        "    ],\n",
        "    voting='soft'   # soft voting uses predicted probabilities\n",
        ")\n",
        "\n",
        "# 9. Train the ensemble model\n",
        "voting_model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 10. Predict\n",
        "y_pred = voting_model.predict(X_test_vec)\n",
        "\n",
        "# 11. Evaluate\n",
        "print(\"Voting Classifier (SVM + RF + XGB)\\n\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQMGPPawWAEv",
        "outputId": "6fbf8afe-c995-4183-bcc3-6aee55560ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:34:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier (SVM + RF + XGB)\n",
            "\n",
            "Accuracy: 0.6203630623520127\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.38      0.47       553\n",
            "           1       0.63      0.81      0.71       714\n",
            "\n",
            "    accuracy                           0.62      1267\n",
            "   macro avg       0.61      0.59      0.59      1267\n",
            "weighted avg       0.62      0.62      0.60      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gated Recurrent Unit"
      ],
      "metadata": {
        "id": "WvyLyxsLfhN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "# 1. Load your train, valid, and test data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv', sep='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv', sep='\\t', header=None)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv', sep='\\t', header=None)\n",
        "\n",
        "# 2. Set column names\n",
        "columns = [\n",
        "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\",\n",
        "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
        "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "]\n",
        "train_df.columns = columns\n",
        "valid_df.columns = columns\n",
        "test_df.columns = columns\n",
        "\n",
        "# 3. Label mapping (same as before)\n",
        "label_mapping = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 0,\n",
        "    'barely-true': 0,\n",
        "    'half-true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'true': 1\n",
        "}\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "valid_df['label'] = valid_df['label'].map(label_mapping)\n",
        "test_df['label'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# 4. Combine train + valid\n",
        "full_train_df = pd.concat([train_df, valid_df])\n",
        "\n",
        "# 5. Get texts and labels\n",
        "X_train = full_train_df['statement'].astype(str)\n",
        "y_train = full_train_df['label']\n",
        "\n",
        "X_test = test_df['statement'].astype(str)\n",
        "y_test = test_df['label']\n",
        "\n",
        "# 6. Tokenization and Padding\n",
        "vocab_size = 10000  # You can change based on your dataset size\n",
        "maxlen = 100        # Maximum number of words per statement\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n",
        "\n",
        "# 7. Build GRU Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=maxlen),\n",
        "    GRU(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# model = Sequential([\n",
        "#     Embedding(vocab_size, 128, input_length=maxlen),\n",
        "#     Bidirectional(GRU(64)),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(32, activation='relu'),\n",
        "#     Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 8. Train Model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 9. Evaluate\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"GRU Model Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 10. Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEy9_b4tflmO",
        "outputId": "5d0225dd-864e-495f-afcf-c141dbbf22fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "289/289 - 10s - 34ms/step - accuracy: 0.5593 - loss: 0.6872 - val_accuracy: 0.5323 - val_loss: 0.6930\n",
            "Epoch 2/10\n",
            "289/289 - 3s - 10ms/step - accuracy: 0.5632 - loss: 0.6869 - val_accuracy: 0.5323 - val_loss: 0.6942\n",
            "Epoch 3/10\n",
            "289/289 - 2s - 8ms/step - accuracy: 0.5632 - loss: 0.6867 - val_accuracy: 0.5323 - val_loss: 0.6914\n",
            "Epoch 4/10\n",
            "289/289 - 2s - 8ms/step - accuracy: 0.5633 - loss: 0.6857 - val_accuracy: 0.5323 - val_loss: 0.6911\n",
            "Epoch 5/10\n",
            "289/289 - 3s - 9ms/step - accuracy: 0.5634 - loss: 0.6858 - val_accuracy: 0.5323 - val_loss: 0.6953\n",
            "Epoch 6/10\n",
            "289/289 - 3s - 10ms/step - accuracy: 0.5634 - loss: 0.6854 - val_accuracy: 0.5323 - val_loss: 0.6911\n",
            "Epoch 7/10\n",
            "289/289 - 5s - 16ms/step - accuracy: 0.5623 - loss: 0.6861 - val_accuracy: 0.5323 - val_loss: 0.6926\n",
            "GRU Model Test Accuracy: 0.5635\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       553\n",
            "           1       0.56      1.00      0.72       714\n",
            "\n",
            "    accuracy                           0.56      1267\n",
            "   macro avg       0.28      0.50      0.36      1267\n",
            "weighted avg       0.32      0.56      0.41      1267\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#gru+cnn"
      ],
      "metadata": {
        "id": "i9G9aeuSiYJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. Load and preprocess your data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv', sep='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv', sep='\\t', header=None)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv', sep='\\t', header=None)\n",
        "\n",
        "columns = [\n",
        "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\",\n",
        "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
        "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "]\n",
        "train_df.columns = columns\n",
        "valid_df.columns = columns\n",
        "test_df.columns = columns\n",
        "\n",
        "label_mapping = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 0,\n",
        "    'barely-true': 0,\n",
        "    'half-true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'true': 1\n",
        "}\n",
        "\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "valid_df['label'] = valid_df['label'].map(label_mapping)\n",
        "test_df['label'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# Combine train + valid\n",
        "full_train_df = pd.concat([train_df, valid_df])\n",
        "\n",
        "X_train = full_train_df['statement'].astype(str)\n",
        "y_train = full_train_df['label']\n",
        "\n",
        "X_test = test_df['statement'].astype(str)\n",
        "y_test = test_df['label']\n",
        "\n",
        "# Tokenization and Padding\n",
        "vocab_size = 10000\n",
        "maxlen = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n",
        "\n",
        "# 2. Build GRU + CNN Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=maxlen),\n",
        "    GRU(64, return_sequences=True),    # Set return_sequences=True because CNN needs the full sequence\n",
        "    Conv1D(64, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),               # Reduce dimensions after convolution\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train Model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 4. Evaluate\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"GRU + CNN Model Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 5. Classification Report\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFSQQJloiXzN",
        "outputId": "286e2a57-b6bf-423a-b406-7914f48fee1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289/289 - 38s - 131ms/step - accuracy: 0.5727 - loss: 0.6753 - val_accuracy: 0.6130 - val_loss: 0.6530\n",
            "Epoch 2/10\n",
            "289/289 - 41s - 141ms/step - accuracy: 0.6704 - loss: 0.6098 - val_accuracy: 0.6022 - val_loss: 0.6652\n",
            "Epoch 3/10\n",
            "289/289 - 40s - 140ms/step - accuracy: 0.7682 - loss: 0.4892 - val_accuracy: 0.5874 - val_loss: 0.7124\n",
            "Epoch 4/10\n",
            "289/289 - 48s - 166ms/step - accuracy: 0.8471 - loss: 0.3518 - val_accuracy: 0.5918 - val_loss: 0.8980\n",
            "GRU + CNN Model Test Accuracy: 0.5675\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.56      0.53       553\n",
            "           1       0.63      0.58      0.60       714\n",
            "\n",
            "    accuracy                           0.57      1267\n",
            "   macro avg       0.57      0.57      0.56      1267\n",
            "weighted avg       0.57      0.57      0.57      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRU+RNN"
      ],
      "metadata": {
        "id": "WDQ1RSUhi_nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. Load and preprocess your data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv', sep='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv', sep='\\t', header=None)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv', sep='\\t', header=None)\n",
        "\n",
        "columns = [\n",
        "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\",\n",
        "    \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\",\n",
        "    \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "]\n",
        "train_df.columns = columns\n",
        "valid_df.columns = columns\n",
        "test_df.columns = columns\n",
        "\n",
        "label_mapping = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 0,\n",
        "    'barely-true': 0,\n",
        "    'half-true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'true': 1\n",
        "}\n",
        "\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "valid_df['label'] = valid_df['label'].map(label_mapping)\n",
        "test_df['label'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# Combine train + valid\n",
        "full_train_df = pd.concat([train_df, valid_df])\n",
        "\n",
        "X_train = full_train_df['statement'].astype(str)\n",
        "y_train = full_train_df['label']\n",
        "\n",
        "X_test = test_df['statement'].astype(str)\n",
        "y_test = test_df['label']\n",
        "\n",
        "# Tokenization and Padding\n",
        "vocab_size = 10000\n",
        "maxlen = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n",
        "\n",
        "# 2. Build GRU + RNN Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=maxlen),\n",
        "    GRU(64, return_sequences=True),    # GRU captures long-range dependencies\n",
        "    SimpleRNN(32, return_sequences=False),  # SimpleRNN captures short-range dependencies\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train Model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 4. Evaluate\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"GRU + RNN Model Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 5. Classification Report\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hy9w51WjCsa",
        "outputId": "d9ebcdb0-7eea-4b31-f717-c173928c605c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "289/289 - 42s - 146ms/step - accuracy: 0.5515 - loss: 0.6891 - val_accuracy: 0.5323 - val_loss: 0.6911\n",
            "Epoch 2/10\n",
            "289/289 - 32s - 110ms/step - accuracy: 0.5597 - loss: 0.6880 - val_accuracy: 0.5323 - val_loss: 0.6936\n",
            "Epoch 3/10\n",
            "289/289 - 40s - 137ms/step - accuracy: 0.5597 - loss: 0.6875 - val_accuracy: 0.5323 - val_loss: 0.6979\n",
            "Epoch 4/10\n",
            "289/289 - 33s - 113ms/step - accuracy: 0.5582 - loss: 0.6874 - val_accuracy: 0.5323 - val_loss: 0.6937\n",
            "GRU + RNN Model Test Accuracy: 0.5635\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       553\n",
            "           1       0.56      1.00      0.72       714\n",
            "\n",
            "    accuracy                           0.56      1267\n",
            "   macro avg       0.28      0.50      0.36      1267\n",
            "weighted avg       0.32      0.56      0.41      1267\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TAharasjbNm",
        "outputId": "6ba11d4a-3551-4867-a521-ce6b026375f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}